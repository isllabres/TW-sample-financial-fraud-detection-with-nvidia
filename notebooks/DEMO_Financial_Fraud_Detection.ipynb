{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The objective of this notebook is to showcase the usage of the [___financial-fraud-training___ container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/cugraph/containers/financial-fraud-training) and how to deploy the produced trained models on [NVIDIA Dynamo-Triton](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver).\n",
    "- We use [IBM TabFormer](https://github.com/IBM/TabFormer) as an example dataset and the dataset is preprocess before model training\n",
    "\n",
    "NOTE:\n",
    "* The preprocessing code is written specifically for the TabFormer dataset and will not work with other datasets.\n",
    "* Additionally, a familiarity with [Jupyter](https://docs.jupyter.org/en/latest/what_is_jupyter.html) is assumed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup (Sagemaker studio)\n",
    "This Notebook is designed to work in a Sagemaker studio jupyter lab notebook\n",
    "\n",
    "Please create a Conda environment and add that to the notebook - See the [README - Setup Development Environment section](../README.md) file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:13.714496Z",
     "iopub.status.busy": "2025-09-29T16:05:13.714235Z",
     "iopub.status.idle": "2025-09-29T16:05:13.899396Z",
     "shell.execute_reply": "2025-09-29T16:05:13.898724Z",
     "shell.execute_reply.started": "2025-09-29T16:05:13.714474Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activate the env in the kernel\n",
    "\n",
    "Now choose the `fraud_blueprint_env` kernel from within jupyterlab for this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:14.680520Z",
     "iopub.status.busy": "2025-09-29T16:05:14.680251Z",
     "iopub.status.idle": "2025-09-29T16:05:14.760961Z",
     "shell.execute_reply": "2025-09-29T16:05:14.760455Z",
     "shell.execute_reply.started": "2025-09-29T16:05:14.680496Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the AWS Environment variables from the infrastructure stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:15.255667Z",
     "iopub.status.busy": "2025-09-29T16:05:15.255406Z",
     "iopub.status.idle": "2025-09-29T16:05:15.260735Z",
     "shell.execute_reply": "2025-09-29T16:05:15.260226Z",
     "shell.execute_reply.started": "2025-09-29T16:05:15.255646Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add the \"src\" directory to the search path\n",
    "src_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "sys.path.insert(0, src_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.aws import get_cfn_output, get_inference_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:15.718272Z",
     "iopub.status.busy": "2025-09-29T16:05:15.718024Z",
     "iopub.status.idle": "2025-09-29T16:05:16.089144Z",
     "shell.execute_reply": "2025-09-29T16:05:16.088631Z",
     "shell.execute_reply.started": "2025-09-29T16:05:15.718250Z"
    }
   },
   "outputs": [],
   "source": [
    "ssm_client = boto3.client('ssm')\n",
    "cfn_client = boto3.client('cloudformation')\n",
    "elb_client = boto3.client('elbv2', region_name=\"us-east-1\")\n",
    "\n",
    "bucket_name = get_cfn_output(\"NvidiaFraudDetectionBlueprintModelExtractor\", \"SourceBucketName\")\n",
    "\n",
    "sagemaker_training_role = get_cfn_output(\"NvidiaFraudDetectionTrainingRole\", \"SageMakerRoleArn\")\n",
    "\n",
    "training_repo = get_cfn_output(\"NvidiaFraudDetectionTrainingImageRepo\", \"TrainingImageRepoUri\")\n",
    "\n",
    "inference_host = get_inference_host()\n",
    "\n",
    "print(f\"Bucket Name: {bucket_name}\")\n",
    "print(f\"Training Role: {sagemaker_training_role}\")\n",
    "print(f\"Training Repo: {training_repo}\")\n",
    "print(f\"Inference Host: {inference_host}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Step 1: Get and Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Local\n",
    "1. Download the dataset: https://ibm.ent.box.com/v/tabformer-data/folder/130747715605\n",
    "2. untar and uncompreess the file: `tar -xvzf ./transactions.tgz`\n",
    "3. Put card_transaction.v1.csv in in the `data/TabFormer/raw` folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Brev \n",
    "1. Download the dataset: https://ibm.ent.box.com/v/tabformer-data/folder/130747715605\n",
    "2. In the Jupyter notebook window, use the \"File Browser\" section to the data/Tabformer/raw folder\n",
    "3. Drag-and-drop the \"transactions.tgz\" file into the folder\n",
    "    - There is also an \"upload\" option that displays a file selector\n",
    "    - Please wait for the upload to finish, it could take a while, by lookign at the status indocator at the bottom of the window\n",
    "4. Now uncompress and untar by running the following command\n",
    "    - Note: if somethign goes wrong you will need to delete the file rather than trying to overwrite it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:18.569094Z",
     "iopub.status.busy": "2025-09-29T16:05:18.568832Z",
     "iopub.status.idle": "2025-09-29T16:05:18.732010Z",
     "shell.execute_reply": "2025-09-29T16:05:18.731361Z",
     "shell.execute_reply.started": "2025-09-29T16:05:18.569073Z"
    }
   },
   "outputs": [],
   "source": [
    "# verify that the compressed file was uploaded successfully - the size should be 266M\n",
    "!ls -lh ../data/TabFormer/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:18.997491Z",
     "iopub.status.busy": "2025-09-29T16:05:18.997218Z",
     "iopub.status.idle": "2025-09-29T16:05:19.148346Z",
     "shell.execute_reply": "2025-09-29T16:05:19.147676Z",
     "shell.execute_reply.started": "2025-09-29T16:05:18.997467Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncompress/untar the file\n",
    "!tar xvzf ../data/TabFormer/raw/transactions.tgz -C ../data/TabFormer/raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If__ drag-and-drop is not working, please run the [Download TabFormer](./extra/download-tabformer.ipynb) notebook is the \"extra\" folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data folder structure\n",
    "The goal is to produce the following structure\n",
    "\n",
    "```\n",
    ".\n",
    "    data\n",
    "    └── TabFormer\n",
    "        └── raw\n",
    "            └── card_transaction.v1.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:21.192922Z",
     "iopub.status.busy": "2025-09-29T16:05:21.192647Z",
     "iopub.status.idle": "2025-09-29T16:05:21.196483Z",
     "shell.execute_reply": "2025-09-29T16:05:21.195971Z",
     "shell.execute_reply.started": "2025-09-29T16:05:21.192898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Once the raw data is placed as described above, set the path to the TabFormer directory\n",
    "\n",
    "# Change this path to point to TabFormer data\n",
    "data_root_dir = os.path.abspath('../data/TabFormer/') \n",
    "# Change this path to the directory where you want to save your model\n",
    "model_output_dir = os.path.join(data_root_dir, 'trained_models')\n",
    "\n",
    "# Path to save the trained model\n",
    "os.makedirs(model_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define python function to print directory tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:22.114345Z",
     "iopub.status.busy": "2025-09-29T16:05:22.114089Z",
     "iopub.status.idle": "2025-09-29T16:05:22.118316Z",
     "shell.execute_reply": "2025-09-29T16:05:22.117828Z",
     "shell.execute_reply.started": "2025-09-29T16:05:22.114323Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.plotting import print_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:23.403832Z",
     "iopub.status.busy": "2025-09-29T16:05:23.403580Z",
     "iopub.status.idle": "2025-09-29T16:05:23.407497Z",
     "shell.execute_reply": "2025-09-29T16:05:23.407017Z",
     "shell.execute_reply.started": "2025-09-29T16:05:23.403811Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check if the raw data has been placed properly\n",
    "print_tree(data_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 2: Preprocess the data \n",
    "- Import the Python function for preprocessing the TabFormer data\n",
    "- Call `preprocess_TabFormer` function to prepare the data\n",
    "\n",
    "NOTE: The preprocessing can takes a few minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:29.209127Z",
     "iopub.status.busy": "2025-09-29T16:05:29.208801Z",
     "iopub.status.idle": "2025-09-29T16:07:17.187411Z",
     "shell.execute_reply": "2025-09-29T16:07:17.186820Z",
     "shell.execute_reply.started": "2025-09-29T16:05:29.209108Z"
    }
   },
   "outputs": [],
   "source": [
    "from data_preprocessing.raw_data_processing import load_and_clean_tabformer\n",
    "from data_preprocessing.xgboost_data_generation import generate_xgboost_features\n",
    "from data_preprocessing.gnn_data_generation import generate_gnn_graph_data\n",
    "\n",
    "bundle = load_and_clean_tabformer(\n",
    "    base_path=data_root_dir,              # Folder containing 'raw/card_transaction.v1.csv'\n",
    "    csv_name=\"card_transaction.v1.csv\"\n",
    ")\n",
    "\n",
    "xgb_transformer, columns_of_transformed_data = generate_xgboost_features(\n",
    "    cleaned_data_bundle=bundle,\n",
    "    output_dir=os.path.join(data_root_dir, \"xgb\"),\n",
    "    data_split_year=2018)\n",
    "\n",
    "user_mask_map, mx_mask_map, tx_mask_map = generate_gnn_graph_data(\n",
    "    cleaned_data_bundle=bundle, \n",
    "    output_dir=os.path.join(data_root_dir, \"gnn\"),\n",
    "    data_split_year=2018, \n",
    "    xgb_transformer=xgb_transformer, \n",
    "    columns_of_transformed_txs=columns_of_transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:07:17.188254Z",
     "iopub.status.busy": "2025-09-29T16:07:17.188059Z",
     "iopub.status.idle": "2025-09-29T16:07:17.191760Z",
     "shell.execute_reply": "2025-09-29T16:07:17.191324Z",
     "shell.execute_reply.started": "2025-09-29T16:07:17.188236Z"
    }
   },
   "outputs": [],
   "source": [
    "print_tree(data_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Summary\n",
    "Summarize the bipartite graph structure created by preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import summarize_graph\n",
    "# Run the summary\n",
    "gnn_dir = os.path.join(data_root_dir, \"gnn\")\n",
    "graph_stats = summarize_graph(gnn_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:07:17.192751Z",
     "iopub.status.busy": "2025-09-29T16:07:17.192511Z",
     "iopub.status.idle": "2025-09-29T16:07:23.623610Z",
     "shell.execute_reply": "2025-09-29T16:07:23.622894Z",
     "shell.execute_reply.started": "2025-09-29T16:07:17.192733Z"
    }
   },
   "outputs": [],
   "source": [
    "# copy data to S3 to get pulled during training\n",
    "! aws s3 sync \"../data/TabFormer/\" s3://$bucket_name/data/ --exclude \"*/test_gnn\" --force --delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Step 3:  Now train the model using the financial-fraud-training container\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training configuration file\n",
    "NOTE: Training configuration file must conform to schema defined in docs (to be updated.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important: Models and configuration files needed for deployment using NVIDIA Dynamo-Triton will be saved in model-repository under the folder that is mounted in /trained_models inside the container__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:09:52.679791Z",
     "iopub.status.busy": "2025-09-29T16:09:52.679511Z",
     "iopub.status.idle": "2025-09-29T16:09:52.683485Z",
     "shell.execute_reply": "2025-09-29T16:09:52.682940Z",
     "shell.execute_reply.started": "2025-09-29T16:09:52.679764Z"
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"config/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "training_config = config[\"training\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the training configuration as a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:09:53.598930Z",
     "iopub.status.busy": "2025-09-29T16:09:53.598668Z",
     "iopub.status.idle": "2025-09-29T16:09:54.580158Z",
     "shell.execute_reply": "2025-09-29T16:09:54.579510Z",
     "shell.execute_reply.started": "2025-09-29T16:09:53.598908Z"
    }
   },
   "outputs": [],
   "source": [
    "training_config_file_name = 'training_config.json'\n",
    "\n",
    "with open(os.path.join(training_config_file_name), 'w') as json_file:\n",
    "    json.dump(training_config, json_file, indent=4)\n",
    "\n",
    "# clone config to S3\n",
    "! aws s3 cp ./training_config.json s3://$bucket_name/config/training_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model using financial_fraud_training container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:50:16.411615Z",
     "iopub.status.busy": "2025-09-29T16:50:16.411338Z",
     "iopub.status.idle": "2025-09-29T16:50:17.029736Z",
     "shell.execute_reply": "2025-09-29T16:50:17.029201Z",
     "shell.execute_reply.started": "2025-09-29T16:50:16.411593Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')  # Adjust path if needed to find src/\n",
    "\n",
    "from src.training import SageMakerTrainingJob\n",
    "from src.training.sagemaker_training import SageMakerTrainingConfig\n",
    "\n",
    "# Create configuration\n",
    "config = SageMakerTrainingConfig(\n",
    "    bucket_name=bucket_name,\n",
    "    sagemaker_training_role=sagemaker_training_role,\n",
    "    training_repo=training_repo,\n",
    "    # Optional overrides (defaults shown):\n",
    "    # instance_type=\"ml.g5.xlarge\",\n",
    "    # cuda_compat_version=\"cuda-compat-13-0\",\n",
    ")\n",
    "\n",
    "# Launch training job\n",
    "training_job = SageMakerTrainingJob(config)\n",
    "training_job_arn = training_job.launch()\n",
    "\n",
    "# Access job name if needed\n",
    "print(f\"Job name: {training_job.training_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure that the training job succeeds\n",
    "According to the training configuration file defined earlier, if the training runs successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:50:17.257552Z",
     "iopub.status.busy": "2025-09-29T16:50:17.257309Z",
     "iopub.status.idle": "2025-09-29T16:59:48.463899Z",
     "shell.execute_reply": "2025-09-29T16:59:48.463412Z",
     "shell.execute_reply.started": "2025-09-29T16:50:17.257533Z"
    }
   },
   "outputs": [],
   "source": [
    "final_status = training_job.poll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Step 4:  Serve your python backend model using NVIDIA Dynamo-Triton\n",
    "__!Important__: Change MODEL_REPO_PATH to point to `{model_output_dir}` / `python_backend_model_repository` if you used a different path in your training configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install NVIDIA Dynamo-Triton Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'tritonclient[all]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as triton_grpc\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient import utils as triton_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replace HOST with the actual URL where your NVIDIA Dynamo-Triton server is hosted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = inference_host\n",
    "HTTP_PORT = 80\n",
    "GRPC_PORT = 8006\n",
    "METRICS_PORT = 8007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve your models with NVIDIA Dynamo-Triton\n",
    "\n",
    "With the infrastructure repo deployed, we have a Lambda function waiting for the training job to complete and for the models to be output to `s3://ml-on-containers-<accountnumber>/output` and then, they'll get extracted to a different bucket to be served by the inference host setup by our infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs for GRPC and HTTP request to the inference server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_grpc = triton_grpc.InferenceServerClient(url=f'{HOST}:{GRPC_PORT}')\n",
    "client_http = httpclient.InferenceServerClient(url=f'{HOST}:{HTTP_PORT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here’s an example of how to prepare data for inference, using random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tritonclient.http import InferenceServerClient, InferInput, InferRequestedOutput\n",
    "\n",
    "def make_example_request():\n",
    "    # -- example sizes --\n",
    "    num_merchants = 5\n",
    "    num_users   = 7\n",
    "    num_edges   = 3\n",
    "    merchant_feature_dim = 24\n",
    "    user_feature_dim = 13\n",
    "    user_to_merchant_feature_dim = 38\n",
    "\n",
    "    # -- 1) features --\n",
    "    x_merchant = np.random.randn(num_merchants, merchant_feature_dim).astype(np.float32)\n",
    "    x_user   = np.random.randn(num_users, user_feature_dim).astype(np.float32)\n",
    "\n",
    "    # -- 2) shap flag and masks --\n",
    "    compute_shap          = np.array([True], dtype=np.bool_)\n",
    "    feature_mask_merchant   = np.random.randint(0,2, size=(merchant_feature_dim,), dtype=np.int32)\n",
    "    feature_mask_user     = np.random.randint(0,2, size=(user_feature_dim,), dtype=np.int32)\n",
    "\n",
    "    # -- 3) edges: index [2, num_edges] and attributes [num_edges,user_to_merchant_feature_dim] --\n",
    "    edge_index_user_to_merchant = np.vstack([\n",
    "        np.random.randint(0, num_users,   size=(num_edges,)),\n",
    "        np.random.randint(0, num_merchants, size=(num_edges,))\n",
    "    ]).astype(np.int64)\n",
    "    \n",
    "    edge_attr_user_to_merchant = np.random.randn(num_edges, user_to_merchant_feature_dim).astype(np.float32)\n",
    "\n",
    "    feature_mask_user_to_merchant =  np.random.randint(0,2, size=(user_to_merchant_feature_dim,), dtype=np.int32)\n",
    "\n",
    "    return {\n",
    "        \"x_merchant\": x_merchant,\n",
    "        \"x_user\": x_user,\n",
    "        \"COMPUTE_SHAP\": compute_shap,\n",
    "        \"feature_mask_merchant\": feature_mask_merchant,\n",
    "        \"feature_mask_user\": feature_mask_user,\n",
    "        \"edge_index_user_to_merchant\": edge_index_user_to_merchant,\n",
    "        \"edge_attr_user_to_merchant\": edge_attr_user_to_merchant,\n",
    "        \"edge_feature_mask_user_to_merchant\": feature_mask_user_to_merchant\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_and_send_inference_request(data):\n",
    "\n",
    "    # Connect to Triton\n",
    "    client = httpclient.InferenceServerClient(url=f'{HOST}:{HTTP_PORT}')\n",
    "\n",
    "    # Prepare Inputs\n",
    "\n",
    "    inputs = []\n",
    "    def _add_input(name, arr, dtype):\n",
    "        inp = InferInput(name, arr.shape, datatype=dtype)\n",
    "        inp.set_data_from_numpy(arr)\n",
    "        inputs.append(inp)\n",
    "\n",
    "    for key, value in data.items():\n",
    "        if key.startswith(\"x_\"):\n",
    "            dtype = \"FP32\"\n",
    "        elif key.startswith(\"feature_mask_\"):\n",
    "            dtype = \"INT32\"\n",
    "        elif key.startswith(\"edge_feature_mask_\"):\n",
    "            dtype = \"INT32\"            \n",
    "        elif key.startswith(\"edge_index_\"):\n",
    "            dtype = \"INT64\"\n",
    "        elif key.startswith(\"edge_attr_\"):\n",
    "            dtype = \"FP32\"\n",
    "        elif key == \"COMPUTE_SHAP\":\n",
    "            dtype = \"BOOL\"\n",
    "        else:\n",
    "            continue  # skip things we don't care about\n",
    "\n",
    "        _add_input(key, value, dtype)\n",
    "\n",
    "\n",
    "    # Outputs\n",
    "\n",
    "    outputs = [InferRequestedOutput(\"PREDICTION\")]\n",
    "\n",
    "    for key in data:\n",
    "        if key.startswith(\"x_\"):\n",
    "            node = key[len(\"x_\"):]  # extract node name\n",
    "            outputs.append(InferRequestedOutput(f\"shap_values_{node}\"))\n",
    "        elif key.startswith(\"edge_attr_\"):\n",
    "            edge_name = key[len(\"edge_attr_\"):]  # extract edge name\n",
    "            outputs.append(InferRequestedOutput(f\"shap_values_{edge_name}\"))\n",
    "    \n",
    "    # Send request\n",
    "\n",
    "    model_name=\"prediction_and_shapley\"\n",
    "    response = client.infer(\n",
    "        model_name,\n",
    "        inputs=inputs,\n",
    "        request_id=str(1),\n",
    "        outputs=outputs,\n",
    "        timeout= 3000\n",
    "    )\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    # always include prediction\n",
    "    result[\"PREDICTION\"] = response.as_numpy(\"PREDICTION\")\n",
    "\n",
    "    # add shap values\n",
    "    for key in data:\n",
    "        if key.startswith(\"x_\"):\n",
    "            node = key[len(\"x_\"):]  # e.g. \"merchant\", \"user\"\n",
    "            result[f\"shap_values_{node}\"] = response.as_numpy(f\"shap_values_{node}\")\n",
    "        if key.startswith(\"edge_attr_\"):\n",
    "            edge_name = key[len(\"edge_attr_\"):]  # e.g. (\"user\" \"to\"  \"merchant\")\n",
    "            result[f\"shap_values_{edge_name}\"] = response.as_numpy(f\"shap_values_{edge_name}\")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction without computing Shapley values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read preprocessed input transactions to send query to NVIDIA Dynamo-Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_hetero_graph(gnn_data_dir):\n",
    "    \"\"\"\n",
    "    Reads:\n",
    "      - All node CSVs from nodes/, plus their matching feature masks (<node>_feature_mask.csv)\n",
    "        If missing, a mask of all ones is created (np.int32).\n",
    "      - All edge CSVs from edges/:\n",
    "          base        -> edge_index_<edge> (np.int64)\n",
    "          *_attr.csv  -> edge_attr_<edge>  (np.float32)\n",
    "          *_label.csv -> exactly one -> edge_label_<edge> (DataFrame)\n",
    "    \"\"\"\n",
    "    base = os.path.join(gnn_data_dir, \"test_gnn\")\n",
    "    nodes_dir = os.path.join(base, \"nodes\")\n",
    "    edges_dir = os.path.join(base, \"edges\")\n",
    "\n",
    "    out = {}\n",
    "    node_feature_mask = {}\n",
    "\n",
    "    # --- Nodes: every CSV becomes x_<node>; also read/create feature_mask_<node> ---\n",
    "    if os.path.isdir(nodes_dir):\n",
    "        for fname in os.listdir(nodes_dir):\n",
    "            if fname.lower().endswith(\".csv\") and not fname.lower().endswith(\"_feature_mask.csv\"):\n",
    "                node_name = fname[:-len(\".csv\")]\n",
    "                node_path = os.path.join(nodes_dir, fname)\n",
    "                node_df = pd.read_csv(node_path)\n",
    "                out[f\"x_{node_name}\"] = node_df.to_numpy(dtype=np.float32)\n",
    "\n",
    "                # feature mask file (optional)\n",
    "                mask_fname = f\"{node_name}_feature_mask.csv\"\n",
    "                mask_path = os.path.join(nodes_dir, mask_fname)\n",
    "                if os.path.exists(mask_path):\n",
    "                    mask_df = pd.read_csv(mask_path, header=None)\n",
    "                    node_feature_mask[node_name] = mask_df\n",
    "                    feature_mask = mask_df.to_numpy(dtype=np.int32).ravel()\n",
    "                else:\n",
    "                    # create a must with all zeros\n",
    "                    feature_mask = np.zeros(node_df.shape[1], dtype=np.int32)\n",
    "                out[f\"feature_mask_{node_name}\"] = feature_mask\n",
    "\n",
    "    # --- Edges: group into base, attr, label by filename suffix ---\n",
    "    base_edges = {}\n",
    "    edge_attrs = {}\n",
    "    edge_labels = {}\n",
    "    edge_feature_mask = {}\n",
    "\n",
    "    if os.path.isdir(edges_dir):\n",
    "        for fname in os.listdir(edges_dir):\n",
    "            if not fname.lower().endswith(\".csv\"):\n",
    "                continue\n",
    "            path = os.path.join(edges_dir, fname)\n",
    "            lower = fname.lower()\n",
    "            if lower.endswith(\"_attr.csv\"):\n",
    "                edge_name = fname[:-len(\"_attr.csv\")]\n",
    "                edge_attrs[edge_name] = pd.read_csv(path) #, header=None)\n",
    "            elif lower.endswith(\"_label.csv\"):\n",
    "                edge_name = fname[:-len(\"_label.csv\")]\n",
    "                edge_labels[edge_name] = pd.read_csv(path)\n",
    "            elif lower.endswith(\"_feature_mask.csv\"):\n",
    "                edge_name = fname[:-len(\"_feature_mask.csv\")]\n",
    "                edge_feature_mask[edge_name] = pd.read_csv(path, header=None)\n",
    "            else:\n",
    "                edge_name = fname[:-len(\".csv\")]\n",
    "                base_edges[edge_name] = pd.read_csv(path) #, header=None)\n",
    "\n",
    "\n",
    "\n",
    "    # Enforce: only one label file total\n",
    "    if len(edge_labels) == 0:\n",
    "        raise FileNotFoundError(\"No '*_label.csv' found in edges/. Exactly one label file is required.\")\n",
    "    if len(edge_labels) > 1:\n",
    "        raise ValueError(f\"Found multiple label files: {list(edge_labels.keys())}. Exactly one is allowed.\")\n",
    "\n",
    "    # Build output keys for edges\n",
    "    for edge_name, df in base_edges.items():\n",
    "        out[f\"edge_index_{edge_name}\"] = df.to_numpy(dtype=np.int64).T\n",
    "        if edge_name in edge_attrs:\n",
    "            out[f\"edge_attr_{edge_name}\"] = edge_attrs[edge_name].to_numpy(dtype=np.float32)\n",
    "        if edge_name in edge_feature_mask:\n",
    "            out[f\"edge_feature_mask_{edge_name}\"] = edge_feature_mask[edge_name].to_numpy(dtype=np.int32).ravel()\n",
    "        else:\n",
    "            # create a must with all zeros\n",
    "            out[f\"edge_feature_mask_{edge_name}\"] = np.zeros(edge_attrs[edge_name].shape[1], dtype=np.int32)\n",
    "\n",
    "        \n",
    "\n",
    "    # Add the single label file (kept as DataFrame)\n",
    "    (label_edge_name, label_df), = edge_labels.items()\n",
    "    out[f\"edge_label_{label_edge_name}\"] = label_df\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gnn_data_dir = os.path.join(data_root_dir, \"gnn\")\n",
    "test_data = load_hetero_graph(gnn_data_dir)\n",
    "compute_shap = False\n",
    "result =  prepare_and_send_inference_request(test_data | {\"COMPUTE_SHAP\": np.array([compute_shap], dtype=np.bool_)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result['PREDICTION'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_score_for_batch(y, predictions, decision_threshold = 0.5):\n",
    "    # Apply threshold\n",
    "    y_pred = (predictions > decision_threshold).astype(int)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, zero_division=0)\n",
    "    recall = recall_score(y, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y, y_pred, zero_division=0)\n",
    "\n",
    "    # Confusion matrix\n",
    "    classes = ['Non-Fraud', 'Fraud']\n",
    "    columns = pd.MultiIndex.from_product([[\"Predicted\"], classes])\n",
    "    index = pd.MultiIndex.from_product([[\"Actual\"], classes])\n",
    "\n",
    "    conf_mat = confusion_matrix(y, y_pred)\n",
    "    cm_df = pd.DataFrame(conf_mat, index=index, columns=columns)\n",
    "    print(cm_df)\n",
    "\n",
    "    # Plot the confusion matrix directly\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        y, y_pred, display_labels=classes\n",
    "    )\n",
    "    disp.ax_.set_title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary\n",
    "    print(\"----Summary---\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scores on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision threshold to flag a transaction as fraud\n",
    "#Change to trade-off precision and recall\n",
    "decision_threshold = 0.5\n",
    "y = test_data['edge_label_user_to_merchant'].to_numpy(dtype=np.int32)\n",
    "compute_score_for_batch(y, result['PREDICTION'], decision_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 5: Latency and Throughput Tests\n",
    "\n",
    "Measure inference performance with **realistic request sizes**:\n",
    "- **Single transaction**: 1 edge + connected user/merchant (real-time fraud scoring)\n",
    "- **Small batch**: 10-100 transactions (micro-batch processing)\n",
    "- **Varying batch sizes**: Latency scaling analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import statistics\n",
    "\n",
    "def extract_subgraph(full_data, edge_indices, gnn_data_dir):\n",
    "    \"\"\"\n",
    "    Extract a subgraph containing only the specified edges and their connected nodes.\n",
    "    \n",
    "    Args:\n",
    "        full_data: Full test graph data from load_hetero_graph()\n",
    "        edge_indices: List of edge indices to include\n",
    "        gnn_data_dir: Path to GNN data directory\n",
    "    \n",
    "    Returns:\n",
    "        Subgraph data dict ready for inference\n",
    "    \"\"\"\n",
    "    # Get edge data\n",
    "    edge_index = full_data['edge_index_user_to_merchant']  # shape (2, num_edges)\n",
    "    edge_attr = full_data['edge_attr_user_to_merchant']    # shape (num_edges, 38)\n",
    "    \n",
    "    # Extract selected edges\n",
    "    selected_edge_index = edge_index[:, edge_indices]\n",
    "    selected_edge_attr = edge_attr[edge_indices, :]\n",
    "    \n",
    "    # Find unique users and merchants in selected edges\n",
    "    selected_users = np.unique(selected_edge_index[0, :])\n",
    "    selected_merchants = np.unique(selected_edge_index[1, :])\n",
    "    \n",
    "    # Create mapping from old IDs to new contiguous IDs\n",
    "    user_id_map = {old_id: new_id for new_id, old_id in enumerate(selected_users)}\n",
    "    merchant_id_map = {old_id: new_id for new_id, old_id in enumerate(selected_merchants)}\n",
    "    \n",
    "    # Remap edge indices to new contiguous IDs\n",
    "    remapped_edge_index = np.zeros_like(selected_edge_index)\n",
    "    for i in range(selected_edge_index.shape[1]):\n",
    "        remapped_edge_index[0, i] = user_id_map[selected_edge_index[0, i]]\n",
    "        remapped_edge_index[1, i] = merchant_id_map[selected_edge_index[1, i]]\n",
    "    \n",
    "    # Extract node features for selected nodes\n",
    "    x_user = full_data['x_user'][selected_users, :]\n",
    "    x_merchant = full_data['x_merchant'][selected_merchants, :]\n",
    "    \n",
    "    # Build subgraph data\n",
    "    subgraph = {\n",
    "        'x_user': x_user,\n",
    "        'x_merchant': x_merchant,\n",
    "        'feature_mask_user': full_data['feature_mask_user'],\n",
    "        'feature_mask_merchant': full_data['feature_mask_merchant'],\n",
    "        'edge_index_user_to_merchant': remapped_edge_index,\n",
    "        'edge_attr_user_to_merchant': selected_edge_attr,\n",
    "        'edge_feature_mask_user_to_merchant': full_data['edge_feature_mask_user_to_merchant'],\n",
    "    }\n",
    "    \n",
    "    return subgraph\n",
    "\n",
    "def create_batch_samples(full_data, batch_size, num_samples, gnn_data_dir):\n",
    "    \"\"\"Create multiple subgraph samples of a given batch size.\"\"\"\n",
    "    num_edges = full_data['edge_index_user_to_merchant'].shape[1]\n",
    "    samples = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Randomly select batch_size edges\n",
    "        edge_indices = np.random.choice(num_edges, size=min(batch_size, num_edges), replace=False)\n",
    "        subgraph = extract_subgraph(full_data, edge_indices, gnn_data_dir)\n",
    "        samples.append(subgraph)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def print_subgraph_stats(subgraph, label=\"\"):\n",
    "    \"\"\"Print statistics about a subgraph.\"\"\"\n",
    "    num_users = subgraph['x_user'].shape[0]\n",
    "    num_merchants = subgraph['x_merchant'].shape[0]\n",
    "    num_edges = subgraph['edge_index_user_to_merchant'].shape[1]\n",
    "    print(f\"{label}: {num_edges} transactions, {num_users} users, {num_merchants} merchants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Realistic Test Samples\n",
    "Generate subgraphs of varying sizes to simulate real-world request patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full test data\n",
    "full_test_data = load_hetero_graph(gnn_data_dir)\n",
    "\n",
    "# Create samples for different batch sizes\n",
    "batch_sizes = [1, 10, 50, 100]\n",
    "num_samples_per_size = 500  # Number of samples to create for each batch size\n",
    "\n",
    "test_samples = {}\n",
    "for batch_size in batch_sizes:\n",
    "    samples = create_batch_samples(full_test_data, batch_size, num_samples_per_size, gnn_data_dir)\n",
    "    test_samples[batch_size] = samples\n",
    "    print_subgraph_stats(samples[0], f\"Batch size {batch_size} (example)\")\n",
    "\n",
    "print(f\"\\nCreated {num_samples_per_size} samples for each batch size: {batch_sizes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Transaction Latency (Real-time Fraud Scoring)\n",
    "This is the most critical metric: how fast can we score a single incoming transaction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency_for_samples(samples, compute_shap=False, warmup=5):\n",
    "    \"\"\"Measure latency for a list of subgraph samples.\"\"\"\n",
    "    latencies = []\n",
    "    \n",
    "    # Warmup with first sample\n",
    "    warmup_data = samples[0] | {\"COMPUTE_SHAP\": np.array([compute_shap], dtype=np.bool_)}\n",
    "    for _ in range(warmup):\n",
    "        prepare_and_send_inference_request(warmup_data)\n",
    "    \n",
    "    # Measure each sample\n",
    "    for i, sample in enumerate(samples):\n",
    "        request_data = sample | {\"COMPUTE_SHAP\": np.array([compute_shap], dtype=np.bool_)}\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        prepare_and_send_inference_request(request_data)\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        latencies.append((end - start) * 1000)  # Convert to ms\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Completed {i + 1}/{len(samples)} requests...\")\n",
    "    \n",
    "    return latencies\n",
    "\n",
    "def print_latency_stats(latencies, label=\"\"):\n",
    "    \"\"\"Print latency statistics.\"\"\"\n",
    "    if len(latencies) < 2:\n",
    "        print(f\"Not enough samples for statistics (got {len(latencies)})\")\n",
    "        return\n",
    "        \n",
    "    latencies_sorted = sorted(latencies)\n",
    "    n = len(latencies)\n",
    "    \n",
    "    print(f\"\\n{'='*55}\")\n",
    "    print(f\"  {label}\")\n",
    "    print(f\"{'='*55}\")\n",
    "    print(f\"  Requests:    {n}\")\n",
    "    print(f\"  Mean:        {statistics.mean(latencies):>8.2f} ms\")\n",
    "    print(f\"  Std Dev:     {statistics.stdev(latencies):>8.2f} ms\")\n",
    "    print(f\"  Min:         {min(latencies):>8.2f} ms\")\n",
    "    print(f\"  Max:         {max(latencies):>8.2f} ms\")\n",
    "    print(f\"  P50:         {latencies_sorted[int(n * 0.50)]:>8.2f} ms\")\n",
    "    print(f\"  P90:         {latencies_sorted[int(n * 0.90)]:>8.2f} ms\")\n",
    "    print(f\"  P95:         {latencies_sorted[int(n * 0.95)]:>8.2f} ms\")\n",
    "    print(f\"  P99:         {latencies_sorted[min(int(n * 0.99), n-1)]:>8.2f} ms\")\n",
    "    print(f\"{'='*55}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure single transaction latency (batch_size=1)\n",
    "print(\"Measuring SINGLE TRANSACTION latency (batch_size=1)...\")\n",
    "single_tx_latencies = measure_latency_for_samples(test_samples[1], compute_shap=False)\n",
    "print_latency_stats(single_tx_latencies, \"Single Transaction Latency (without SHAP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latency vs Batch Size\n",
    "How does latency scale as we increase the number of transactions per request?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure latency for different batch sizes\n",
    "latency_by_batch_size = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"\\nMeasuring latency for batch_size={batch_size}...\")\n",
    "    latencies = measure_latency_for_samples(test_samples[batch_size], compute_shap=False)\n",
    "    latency_by_batch_size[batch_size] = latencies\n",
    "    print_latency_stats(latencies, f\"Batch Size = {batch_size} transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throughput Test (Concurrent Single-Transaction Requests)\n",
    "Simulate multiple users submitting transactions simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_throughput(samples, num_workers=10, compute_shap=False):\n",
    "    \"\"\"Measure throughput with concurrent requests using realistic single-tx samples.\"\"\"\n",
    "    latencies = []\n",
    "    errors = 0\n",
    "    \n",
    "    # Cycle through samples for each request\n",
    "    sample_cycle = samples * (len(samples) // num_workers + 1)  # Ensure enough samples\n",
    "    \n",
    "    def send_request(sample):\n",
    "        try:\n",
    "            request_data = sample | {\"COMPUTE_SHAP\": np.array([compute_shap], dtype=np.bool_)}\n",
    "            start = time.perf_counter()\n",
    "            prepare_and_send_inference_request(request_data)\n",
    "            end = time.perf_counter()\n",
    "            return (end - start) * 1000, None\n",
    "        except Exception as e:\n",
    "            return None, str(e)\n",
    "    \n",
    "    num_requests = len(samples)\n",
    "    print(f\"Running throughput test: {num_requests} requests, {num_workers} concurrent workers...\")\n",
    "    \n",
    "    overall_start = time.perf_counter()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = [executor.submit(send_request, sample_cycle[i]) for i in range(num_requests)]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            latency, error = future.result()\n",
    "            if error:\n",
    "                errors += 1\n",
    "            else:\n",
    "                latencies.append(latency)\n",
    "    \n",
    "    overall_end = time.perf_counter()\n",
    "    total_time = overall_end - overall_start\n",
    "    \n",
    "    successful = len(latencies)\n",
    "    throughput = successful / total_time\n",
    "    \n",
    "    print(f\"\\n{'='*55}\")\n",
    "    print(f\"  Throughput Test Results ({num_workers} workers)\")\n",
    "    print(f\"{'='*55}\")\n",
    "    print(f\"  Total requests:     {num_requests}\")\n",
    "    print(f\"  Successful:         {successful}\")\n",
    "    print(f\"  Errors:             {errors}\")\n",
    "    print(f\"  Total time:         {total_time:.2f} s\")\n",
    "    print(f\"  Throughput:         {throughput:.2f} tx/s\")\n",
    "    if latencies:\n",
    "        print(f\"  Avg latency:        {statistics.mean(latencies):.2f} ms\")\n",
    "        print(f\"  P95 latency:        {sorted(latencies)[int(len(latencies) * 0.95)]:.2f} ms\")\n",
    "    print(f\"{'='*55}\")\n",
    "    \n",
    "    return throughput, latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test throughput with different concurrency levels using single-transaction requests\n",
    "concurrency_levels = [1, 5, 10, 20]\n",
    "throughput_results = {}\n",
    "\n",
    "for num_workers in concurrency_levels:\n",
    "    throughput, _ = measure_throughput(test_samples[1], num_workers=num_workers, compute_shap=False)\n",
    "    throughput_results[num_workers] = throughput\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 1. Latency vs Batch Size (box plot)\n",
    "ax1 = axes[0]\n",
    "batch_data = [latency_by_batch_size[bs] for bs in batch_sizes]\n",
    "bp = ax1.boxplot(batch_data, labels=[str(bs) for bs in batch_sizes], patch_artist=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('steelblue')\n",
    "    patch.set_alpha(0.7)\n",
    "ax1.set_xlabel('Batch Size (transactions)', fontsize=11)\n",
    "ax1.set_ylabel('Latency (ms)', fontsize=11)\n",
    "ax1.set_title('Latency vs Batch Size', fontsize=12)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Throughput vs Concurrency\n",
    "ax2 = axes[1]\n",
    "workers = list(throughput_results.keys())\n",
    "throughputs = list(throughput_results.values())\n",
    "bars = ax2.bar(workers, throughputs, color='coral', edgecolor='black', alpha=0.8)\n",
    "ax2.set_xlabel('Concurrent Workers', fontsize=11)\n",
    "ax2.set_ylabel('Throughput (tx/second)', fontsize=11)\n",
    "ax2.set_title('Throughput vs Concurrency\\n(single-tx requests)', fontsize=12)\n",
    "ax2.set_xticks(workers)\n",
    "for bar, t in zip(bars, throughputs):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{t:.1f}', ha='center', fontsize=9)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Single Transaction Latency Distribution\n",
    "ax3 = axes[2]\n",
    "ax3.hist(single_tx_latencies, bins=20, color='seagreen', edgecolor='black', alpha=0.7)\n",
    "mean_lat = statistics.mean(single_tx_latencies)\n",
    "p95_lat = sorted(single_tx_latencies)[int(len(single_tx_latencies) * 0.95)]\n",
    "ax3.axvline(mean_lat, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_lat:.1f}ms')\n",
    "ax3.axvline(p95_lat, color='orange', linestyle='--', linewidth=2, label=f'P95: {p95_lat:.1f}ms')\n",
    "ax3.set_xlabel('Latency (ms)', fontsize=11)\n",
    "ax3.set_ylabel('Frequency', fontsize=11)\n",
    "ax3.set_title('Single Transaction Latency\\nDistribution', fontsize=12)\n",
    "ax3.legend()\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build summary table comparing latency across batch sizes\n",
    "summary_rows = []\n",
    "for batch_size in batch_sizes:\n",
    "    lats = latency_by_batch_size[batch_size]\n",
    "    lats_sorted = sorted(lats)\n",
    "    n = len(lats)\n",
    "    summary_rows.append({\n",
    "        'Batch Size': batch_size,\n",
    "        'Mean (ms)': f\"{statistics.mean(lats):.2f}\",\n",
    "        'P50 (ms)': f\"{lats_sorted[int(n * 0.50)]:.2f}\",\n",
    "        'P95 (ms)': f\"{lats_sorted[int(n * 0.95)]:.2f}\",\n",
    "        'P99 (ms)': f\"{lats_sorted[min(int(n * 0.99), n-1)]:.2f}\",\n",
    "        'Min (ms)': f\"{min(lats):.2f}\",\n",
    "        'Max (ms)': f\"{max(lats):.2f}\",\n",
    "        'Latency/Tx (ms)': f\"{statistics.mean(lats) / batch_size:.2f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.set_index('Batch Size', inplace=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  LATENCY SUMMARY BY BATCH SIZE\")\n",
    "print(\"=\"*70)\n",
    "display(summary_df)\n",
    "\n",
    "# Key insights\n",
    "single_tx_mean = statistics.mean(latency_by_batch_size[1])\n",
    "batch_100_mean = statistics.mean(latency_by_batch_size[100])\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"  - Single transaction P95 latency: {sorted(latency_by_batch_size[1])[int(len(latency_by_batch_size[1]) * 0.95)]:.2f} ms\")\n",
    "print(f\"  - Batching 100 tx reduces per-tx latency by {(1 - (batch_100_mean/100) / single_tx_mean) * 100:.1f}%\")\n",
    "print(f\"  - Max throughput (20 workers): {throughput_results.get(20, 'N/A'):.1f} tx/s\" if 20 in throughput_results else \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
