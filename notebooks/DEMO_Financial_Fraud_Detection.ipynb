{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The objective of this notebook is to showcase the usage of the [___financial-fraud-training___ container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/cugraph/containers/financial-fraud-training) and how to deploy the produced trained models on [NVIDIA Dynamo-Triton](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver).\n",
    "- We use [IBM TabFormer](https://github.com/IBM/TabFormer) as an example dataset and the dataset is preprocess before model training\n",
    "\n",
    "NOTE:\n",
    "* The preprocessing code is written specifically for the TabFormer dataset and will not work with other datasets.\n",
    "* Additionally, a familiarity with [Jupyter](https://docs.jupyter.org/en/latest/what_is_jupyter.html) is assumed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup (Sagemaker studio)\n",
    "This Notebook is designed to work in a Sagemaker studio jupyter lab notebook\n",
    "\n",
    "Please create a Conda environment and add that to the notebook - See the [README - Setup Development Environment section](../README.md) file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:13.714496Z",
     "iopub.status.busy": "2025-09-29T16:05:13.714235Z",
     "iopub.status.idle": "2025-09-29T16:05:13.899396Z",
     "shell.execute_reply": "2025-09-29T16:05:13.898724Z",
     "shell.execute_reply.started": "2025-09-29T16:05:13.714474Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activate the env in the kernel\n",
    "\n",
    "Now choose the `fraud_blueprint_env` kernel from within jupyterlab for this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:14.680520Z",
     "iopub.status.busy": "2025-09-29T16:05:14.680251Z",
     "iopub.status.idle": "2025-09-29T16:05:14.760961Z",
     "shell.execute_reply": "2025-09-29T16:05:14.760455Z",
     "shell.execute_reply.started": "2025-09-29T16:05:14.680496Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the AWS Environment variables from the infrastructure stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:15.255667Z",
     "iopub.status.busy": "2025-09-29T16:05:15.255406Z",
     "iopub.status.idle": "2025-09-29T16:05:15.260735Z",
     "shell.execute_reply": "2025-09-29T16:05:15.260226Z",
     "shell.execute_reply.started": "2025-09-29T16:05:15.255646Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add the \"src\" directory to the search path\n",
    "src_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "sys.path.insert(0, src_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.aws import get_cfn_output, get_inference_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:15.718272Z",
     "iopub.status.busy": "2025-09-29T16:05:15.718024Z",
     "iopub.status.idle": "2025-09-29T16:05:16.089144Z",
     "shell.execute_reply": "2025-09-29T16:05:16.088631Z",
     "shell.execute_reply.started": "2025-09-29T16:05:15.718250Z"
    }
   },
   "outputs": [],
   "source": [
    "ssm_client = boto3.client('ssm')\n",
    "cfn_client = boto3.client('cloudformation')\n",
    "elb_client = boto3.client('elbv2', region_name=\"us-east-1\")\n",
    "\n",
    "bucket_name = get_cfn_output(\"NvidiaFraudDetectionBlueprintModelExtractor\", \"SourceBucketName\")\n",
    "\n",
    "sagemaker_training_role = get_cfn_output(\"NvidiaFraudDetectionTrainingRole\", \"SageMakerRoleArn\")\n",
    "\n",
    "training_repo = get_cfn_output(\"NvidiaFraudDetectionTrainingImageRepo\", \"TrainingImageRepoUri\")\n",
    "\n",
    "inference_host = get_inference_host()\n",
    "\n",
    "print(f\"Bucket Name: {bucket_name}\")\n",
    "print(f\"Training Role: {sagemaker_training_role}\")\n",
    "print(f\"Training Repo: {training_repo}\")\n",
    "print(f\"Inference Host: {inference_host}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Step 1: Get and Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Local\n",
    "1. Download the dataset: https://ibm.ent.box.com/v/tabformer-data/folder/130747715605\n",
    "2. untar and uncompreess the file: `tar -xvzf ./transactions.tgz`\n",
    "3. Put card_transaction.v1.csv in in the `data/TabFormer/raw` folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Brev \n",
    "1. Download the dataset: https://ibm.ent.box.com/v/tabformer-data/folder/130747715605\n",
    "2. In the Jupyter notebook window, use the \"File Browser\" section to the data/Tabformer/raw folder\n",
    "3. Drag-and-drop the \"transactions.tgz\" file into the folder\n",
    "    - There is also an \"upload\" option that displays a file selector\n",
    "    - Please wait for the upload to finish, it could take a while, by lookign at the status indocator at the bottom of the window\n",
    "4. Now uncompress and untar by running the following command\n",
    "    - Note: if somethign goes wrong you will need to delete the file rather than trying to overwrite it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:18.569094Z",
     "iopub.status.busy": "2025-09-29T16:05:18.568832Z",
     "iopub.status.idle": "2025-09-29T16:05:18.732010Z",
     "shell.execute_reply": "2025-09-29T16:05:18.731361Z",
     "shell.execute_reply.started": "2025-09-29T16:05:18.569073Z"
    }
   },
   "outputs": [],
   "source": [
    "# verify that the compressed file was uploaded successfully - the size should be 266M\n",
    "!ls -lh ../data/TabFormer/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:18.997491Z",
     "iopub.status.busy": "2025-09-29T16:05:18.997218Z",
     "iopub.status.idle": "2025-09-29T16:05:19.148346Z",
     "shell.execute_reply": "2025-09-29T16:05:19.147676Z",
     "shell.execute_reply.started": "2025-09-29T16:05:18.997467Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncompress/untar the file\n",
    "!tar xvzf ../data/TabFormer/raw/transactions.tgz -C ../data/TabFormer/raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If__ drag-and-drop is not working, please run the [Download TabFormer](./extra/download-tabformer.ipynb) notebook is the \"extra\" folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data folder structure\n",
    "The goal is to produce the following structure\n",
    "\n",
    "```\n",
    ".\n",
    "    data\n",
    "    └── TabFormer\n",
    "        └── raw\n",
    "            └── card_transaction.v1.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:21.192922Z",
     "iopub.status.busy": "2025-09-29T16:05:21.192647Z",
     "iopub.status.idle": "2025-09-29T16:05:21.196483Z",
     "shell.execute_reply": "2025-09-29T16:05:21.195971Z",
     "shell.execute_reply.started": "2025-09-29T16:05:21.192898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Once the raw data is placed as described above, set the path to the TabFormer directory\n",
    "\n",
    "# Change this path to point to TabFormer data\n",
    "data_root_dir = os.path.abspath('../data/TabFormer/') \n",
    "# Change this path to the directory where you want to save your model\n",
    "model_output_dir = os.path.join(data_root_dir, 'trained_models')\n",
    "\n",
    "# Path to save the trained model\n",
    "os.makedirs(model_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define python function to print directory tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:22.114345Z",
     "iopub.status.busy": "2025-09-29T16:05:22.114089Z",
     "iopub.status.idle": "2025-09-29T16:05:22.118316Z",
     "shell.execute_reply": "2025-09-29T16:05:22.117828Z",
     "shell.execute_reply.started": "2025-09-29T16:05:22.114323Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.plotting import print_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:23.403832Z",
     "iopub.status.busy": "2025-09-29T16:05:23.403580Z",
     "iopub.status.idle": "2025-09-29T16:05:23.407497Z",
     "shell.execute_reply": "2025-09-29T16:05:23.407017Z",
     "shell.execute_reply.started": "2025-09-29T16:05:23.403811Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check if the raw data has been placed properly\n",
    "print_tree(data_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 2: Preprocess the data \n",
    "- Import the Python function for preprocessing the TabFormer data\n",
    "- Call `preprocess_TabFormer` function to prepare the data\n",
    "\n",
    "NOTE: The preprocessing can takes a few minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:05:29.209127Z",
     "iopub.status.busy": "2025-09-29T16:05:29.208801Z",
     "iopub.status.idle": "2025-09-29T16:07:17.187411Z",
     "shell.execute_reply": "2025-09-29T16:07:17.186820Z",
     "shell.execute_reply.started": "2025-09-29T16:05:29.209108Z"
    }
   },
   "outputs": [],
   "source": [
    "from data_preprocessing.raw_data_processing import load_and_clean_tabformer\n",
    "from data_preprocessing.xgboost_data_generation import generate_xgboost_features\n",
    "from data_preprocessing.gnn_data_generation import generate_gnn_graph_data\n",
    "\n",
    "bundle = load_and_clean_tabformer(\n",
    "    base_path=data_root_dir,              # Folder containing 'raw/card_transaction.v1.csv'\n",
    "    csv_name=\"card_transaction.v1.csv\"\n",
    ")\n",
    "\n",
    "xgb_transformer, columns_of_transformed_data = generate_xgboost_features(\n",
    "    cleaned_data_bundle=bundle,\n",
    "    output_dir=os.path.join(data_root_dir, \"xgb\"),\n",
    "    data_split_year=2018)\n",
    "\n",
    "user_mask_map, mx_mask_map, tx_mask_map = generate_gnn_graph_data(\n",
    "    cleaned_data_bundle=bundle, \n",
    "    output_dir=os.path.join(data_root_dir, \"gnn\"),\n",
    "    data_split_year=2018, \n",
    "    xgb_transformer=xgb_transformer, \n",
    "    columns_of_transformed_txs=columns_of_transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:07:17.188254Z",
     "iopub.status.busy": "2025-09-29T16:07:17.188059Z",
     "iopub.status.idle": "2025-09-29T16:07:17.191760Z",
     "shell.execute_reply": "2025-09-29T16:07:17.191324Z",
     "shell.execute_reply.started": "2025-09-29T16:07:17.188236Z"
    }
   },
   "outputs": [],
   "source": [
    "print_tree(data_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Summary\n",
    "Summarize the bipartite graph structure created by preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import summarize_graph\n",
    "# Run the summary\n",
    "gnn_dir = os.path.join(data_root_dir, \"gnn\")\n",
    "graph_stats = summarize_graph(gnn_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:07:17.192751Z",
     "iopub.status.busy": "2025-09-29T16:07:17.192511Z",
     "iopub.status.idle": "2025-09-29T16:07:23.623610Z",
     "shell.execute_reply": "2025-09-29T16:07:23.622894Z",
     "shell.execute_reply.started": "2025-09-29T16:07:17.192733Z"
    }
   },
   "outputs": [],
   "source": [
    "# copy data to S3 to get pulled during training\n",
    "! aws s3 sync \"../data/TabFormer/\" s3://$bucket_name/data/ --exclude \"*/test_gnn\" --force --delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Step 3:  Now train the model using the financial-fraud-training container\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training configuration file\n",
    "NOTE: Training configuration file must conform to schema defined in docs (to be updated.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important: Models and configuration files needed for deployment using NVIDIA Dynamo-Triton will be saved in model-repository under the folder that is mounted in /trained_models inside the container__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:09:52.679791Z",
     "iopub.status.busy": "2025-09-29T16:09:52.679511Z",
     "iopub.status.idle": "2025-09-29T16:09:52.683485Z",
     "shell.execute_reply": "2025-09-29T16:09:52.682940Z",
     "shell.execute_reply.started": "2025-09-29T16:09:52.679764Z"
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"../config/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "training_config = config[\"training\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the training configuration as a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:09:53.598930Z",
     "iopub.status.busy": "2025-09-29T16:09:53.598668Z",
     "iopub.status.idle": "2025-09-29T16:09:54.580158Z",
     "shell.execute_reply": "2025-09-29T16:09:54.579510Z",
     "shell.execute_reply.started": "2025-09-29T16:09:53.598908Z"
    }
   },
   "outputs": [],
   "source": [
    "training_config_file_name = 'training_config.json'\n",
    "\n",
    "with open(os.path.join(training_config_file_name), 'w') as json_file:\n",
    "    json.dump(training_config, json_file, indent=4)\n",
    "\n",
    "# clone config to S3\n",
    "! aws s3 cp ./training_config.json s3://$bucket_name/config/training_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model using financial_fraud_training container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:50:16.411615Z",
     "iopub.status.busy": "2025-09-29T16:50:16.411338Z",
     "iopub.status.idle": "2025-09-29T16:50:17.029736Z",
     "shell.execute_reply": "2025-09-29T16:50:17.029201Z",
     "shell.execute_reply.started": "2025-09-29T16:50:16.411593Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')  # Adjust path if needed to find src/\n",
    "\n",
    "from src.model import SageMakerTrainingJob\n",
    "from src.model.sagemaker_training import SageMakerTrainingConfig\n",
    "\n",
    "# Create configuration\n",
    "config = SageMakerTrainingConfig(\n",
    "    bucket_name=bucket_name,\n",
    "    sagemaker_training_role=sagemaker_training_role,\n",
    "    training_repo=training_repo,\n",
    "    # Optional overrides (defaults shown):\n",
    "    # instance_type=\"ml.g5.xlarge\",\n",
    "    # cuda_compat_version=\"cuda-compat-13-0\",\n",
    ")\n",
    "\n",
    "# Launch training job\n",
    "training_job = SageMakerTrainingJob(config)\n",
    "training_job_arn = training_job.launch()\n",
    "\n",
    "# Access job name if needed\n",
    "print(f\"Job name: {training_job.training_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure that the training job succeeds\n",
    "According to the training configuration file defined earlier, if the training runs successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T16:50:17.257552Z",
     "iopub.status.busy": "2025-09-29T16:50:17.257309Z",
     "iopub.status.idle": "2025-09-29T16:59:48.463899Z",
     "shell.execute_reply": "2025-09-29T16:59:48.463412Z",
     "shell.execute_reply.started": "2025-09-29T16:50:17.257533Z"
    }
   },
   "outputs": [],
   "source": [
    "final_status = training_job.poll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Step 4:  Serve your python backend model using NVIDIA Dynamo-Triton\n",
    "__!Important__: Change MODEL_REPO_PATH to point to `{model_output_dir}` / `python_backend_model_repository` if you used a different path in your training configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install NVIDIA Dynamo-Triton Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'tritonclient[all]' --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tritonclient.http import InferInput, InferRequestedOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as triton_grpc\n",
    "import tritonclient.http as httpclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replace HOST with the actual URL where your NVIDIA Dynamo-Triton server is hosted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = inference_host\n",
    "HTTP_PORT = 80\n",
    "GRPC_PORT = 8006\n",
    "METRICS_PORT = 8007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serve your models with NVIDIA Dynamo-Triton\n",
    "\n",
    "With the infrastructure repo deployed, we have a Lambda function waiting for the training job to complete and for the models to be output to `s3://ml-on-containers-<accountnumber>/output` and then, they'll get extracted to a different bucket to be served by the inference host setup by our infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs for GRPC and HTTP request to the inference server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_grpc = triton_grpc.InferenceServerClient(url=f'{HOST}:{GRPC_PORT}')\n",
    "client_http = httpclient.InferenceServerClient(url=f'{HOST}:{HTTP_PORT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here’s an example of how to prepare data for inference, using random data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction without computing Shapley values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read preprocessed input transactions to send query to NVIDIA Dynamo-Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.sagemaker_inference import prepare_and_send_inference_request, load_hetero_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_data_dir = os.path.join(data_root_dir, \"gnn\")\n",
    "test_data = load_hetero_graph(gnn_data_dir)\n",
    "compute_shap = False\n",
    "result =  prepare_and_send_inference_request(test_data | {\"COMPUTE_SHAP\": np.array([compute_shap], dtype=np.bool_)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result['PREDICTION'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scores on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.metrics import compute_score_for_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision threshold to flag a transaction as fraud\n",
    "#Change to trade-off precision and recall\n",
    "decision_threshold = 0.5\n",
    "y = test_data['edge_label_user_to_merchant'].to_numpy(dtype=np.int32)\n",
    "compute_score_for_batch(y, result['PREDICTION'], decision_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 5: Latency and Throughput Tests\n",
    "\n",
    "Measure inference performance with **realistic request sizes**:\n",
    "- **Single transaction**: 1 edge + connected user/merchant (real-time fraud scoring)\n",
    "- **Small batch**: 10-100 transactions (micro-batch processing)\n",
    "- **Varying batch sizes**: Latency scaling analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Realistic Test Samples\n",
    "Generate subgraphs of varying sizes to simulate real-world request patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.plotting import create_batch_samples, print_subgraph_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full test data\n",
    "full_test_data = load_hetero_graph(gnn_data_dir)\n",
    "\n",
    "# Create samples for different batch sizes\n",
    "batch_sizes = [1, 10, 50, 100]\n",
    "num_samples_per_size = 500  # Number of samples to create for each batch size\n",
    "\n",
    "test_samples = {}\n",
    "for batch_size in batch_sizes:\n",
    "    samples = create_batch_samples(full_test_data, batch_size, num_samples_per_size, gnn_data_dir)\n",
    "    test_samples[batch_size] = samples\n",
    "    print_subgraph_stats(samples[0], f\"Batch size {batch_size} (example)\")\n",
    "\n",
    "print(f\"\\nCreated {num_samples_per_size} samples for each batch size: {batch_sizes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Transaction Latency (Real-time Fraud Scoring)\n",
    "This is the most critical metric: how fast can we score a single incoming transaction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure single transaction latency (batch_size=1)\n",
    "print(\"Measuring SINGLE TRANSACTION latency (batch_size=1)...\")\n",
    "single_tx_latencies = measure_latency_for_samples(test_samples[1], compute_shap=False)\n",
    "print_latency_stats(single_tx_latencies, \"Single Transaction Latency (without SHAP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latency vs Batch Size\n",
    "How does latency scale as we increase the number of transactions per request?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.metrics import measure_latency_for_samples, print_latency_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure latency for different batch sizes\n",
    "latency_by_batch_size = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"\\nMeasuring latency for batch_size={batch_size}...\")\n",
    "    latencies = measure_latency_for_samples(test_samples[batch_size], compute_shap=False)\n",
    "    latency_by_batch_size[batch_size] = latencies\n",
    "    print_latency_stats(latencies, f\"Batch Size = {batch_size} transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throughput Test (Concurrent Single-Transaction Requests)\n",
    "Simulate multiple users submitting transactions simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.metrics import measure_throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test throughput with different concurrency levels using single-transaction requests\n",
    "concurrency_levels = [1, 5, 10, 20]\n",
    "throughput_results = {}\n",
    "\n",
    "for num_workers in concurrency_levels:\n",
    "    throughput, _ = measure_throughput(test_samples[1], num_workers=num_workers, compute_shap=False)\n",
    "    throughput_results[num_workers] = throughput\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 1. Latency vs Batch Size (box plot)\n",
    "ax1 = axes[0]\n",
    "batch_data = [latency_by_batch_size[bs] for bs in batch_sizes]\n",
    "bp = ax1.boxplot(batch_data, labels=[str(bs) for bs in batch_sizes], patch_artist=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('steelblue')\n",
    "    patch.set_alpha(0.7)\n",
    "ax1.set_xlabel('Batch Size (transactions)', fontsize=11)\n",
    "ax1.set_ylabel('Latency (ms)', fontsize=11)\n",
    "ax1.set_title('Latency vs Batch Size', fontsize=12)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Throughput vs Concurrency\n",
    "ax2 = axes[1]\n",
    "workers = list(throughput_results.keys())\n",
    "throughputs = list(throughput_results.values())\n",
    "bars = ax2.bar(workers, throughputs, color='coral', edgecolor='black', alpha=0.8)\n",
    "ax2.set_xlabel('Concurrent Workers', fontsize=11)\n",
    "ax2.set_ylabel('Throughput (tx/second)', fontsize=11)\n",
    "ax2.set_title('Throughput vs Concurrency\\n(single-tx requests)', fontsize=12)\n",
    "ax2.set_xticks(workers)\n",
    "for bar, t in zip(bars, throughputs):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{t:.1f}', ha='center', fontsize=9)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Single Transaction Latency Distribution\n",
    "ax3 = axes[2]\n",
    "ax3.hist(single_tx_latencies, bins=20, color='seagreen', edgecolor='black', alpha=0.7)\n",
    "mean_lat = statistics.mean(single_tx_latencies)\n",
    "p95_lat = sorted(single_tx_latencies)[int(len(single_tx_latencies) * 0.95)]\n",
    "ax3.axvline(mean_lat, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_lat:.1f}ms')\n",
    "ax3.axvline(p95_lat, color='orange', linestyle='--', linewidth=2, label=f'P95: {p95_lat:.1f}ms')\n",
    "ax3.set_xlabel('Latency (ms)', fontsize=11)\n",
    "ax3.set_ylabel('Frequency', fontsize=11)\n",
    "ax3.set_title('Single Transaction Latency\\nDistribution', fontsize=12)\n",
    "ax3.legend()\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build summary table comparing latency across batch sizes\n",
    "summary_rows = []\n",
    "for batch_size in batch_sizes:\n",
    "    lats = latency_by_batch_size[batch_size]\n",
    "    lats_sorted = sorted(lats)\n",
    "    n = len(lats)\n",
    "    summary_rows.append({\n",
    "        'Batch Size': batch_size,\n",
    "        'Mean (ms)': f\"{statistics.mean(lats):.2f}\",\n",
    "        'P50 (ms)': f\"{lats_sorted[int(n * 0.50)]:.2f}\",\n",
    "        'P95 (ms)': f\"{lats_sorted[int(n * 0.95)]:.2f}\",\n",
    "        'P99 (ms)': f\"{lats_sorted[min(int(n * 0.99), n-1)]:.2f}\",\n",
    "        'Min (ms)': f\"{min(lats):.2f}\",\n",
    "        'Max (ms)': f\"{max(lats):.2f}\",\n",
    "        'Latency/Tx (ms)': f\"{statistics.mean(lats) / batch_size:.2f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.set_index('Batch Size', inplace=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  LATENCY SUMMARY BY BATCH SIZE\")\n",
    "print(\"=\"*70)\n",
    "display(summary_df)\n",
    "\n",
    "# Key insights\n",
    "single_tx_mean = statistics.mean(latency_by_batch_size[1])\n",
    "batch_100_mean = statistics.mean(latency_by_batch_size[100])\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"  - Single transaction P95 latency: {sorted(latency_by_batch_size[1])[int(len(latency_by_batch_size[1]) * 0.95)]:.2f} ms\")\n",
    "print(f\"  - Batching 100 tx reduces per-tx latency by {(1 - (batch_100_mean/100) / single_tx_mean) * 100:.1f}%\")\n",
    "print(f\"  - Max throughput (20 workers): {throughput_results.get(20, 'N/A'):.1f} tx/s\" if 20 in throughput_results else \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
